{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting run_analysis function\n",
      "INFO:root:Starting analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create log file at: /Users/davidlee/_Code/rev/cs1/analysis_run_14082024_1903/debug_14082024_1903.log\n",
      "Logging setup complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Columns in facial_similarity: Index(['Unnamed: 0', 'user_id', 'result', 'face_comparison_result',\n",
      "       'created_at', 'facial_image_integrity_result',\n",
      "       'visual_authenticity_result', 'properties', 'attempt_id'],\n",
      "      dtype='object')\n",
      "DEBUG:root:Columns in doc_reports: Index(['Unnamed: 0', 'user_id', 'result', 'visual_authenticity_result',\n",
      "       'image_integrity_result', 'face_detection_result',\n",
      "       'image_quality_result', 'created_at', 'supported_document_result',\n",
      "       'conclusive_document_quality_result', 'colour_picture_result',\n",
      "       'data_validation_result', 'data_consistency_result',\n",
      "       'data_comparison_result', 'attempt_id', 'police_record_result',\n",
      "       'compromised_document_result', 'properties', 'sub_result'],\n",
      "      dtype='object')\n",
      "DEBUG:root:Columns after first merge: Index(['Unnamed: 0_facial', 'user_id', 'result_facial',\n",
      "       'face_comparison_result', 'created_at', 'facial_image_integrity_result',\n",
      "       'visual_authenticity_result_facial', 'properties_facial', 'attempt_id',\n",
      "       'Unnamed: 0_doc', 'result_doc', 'visual_authenticity_result_doc',\n",
      "       'image_integrity_result', 'face_detection_result',\n",
      "       'image_quality_result', 'created_at_doc', 'supported_document_result',\n",
      "       'conclusive_document_quality_result', 'colour_picture_result',\n",
      "       'data_validation_result', 'data_consistency_result',\n",
      "       'data_comparison_result', 'police_record_result',\n",
      "       'compromised_document_result', 'properties_doc', 'sub_result'],\n",
      "      dtype='object')\n",
      "DEBUG:root:Final columns in kyc_data: Index(['Unnamed: 0_facial', 'user_id', 'result_facial',\n",
      "       'face_comparison_result', 'created_at', 'facial_image_integrity_result',\n",
      "       'visual_authenticity_result_facial', 'properties_facial', 'attempt_id',\n",
      "       'Unnamed: 0_doc', 'result_doc', 'visual_authenticity_result_doc',\n",
      "       'image_integrity_result_x', 'face_detection_result_x',\n",
      "       'image_quality_result_x', 'created_at_doc',\n",
      "       'supported_document_result_x', 'conclusive_document_quality_result_x',\n",
      "       'colour_picture_result_x', 'data_validation_result_x',\n",
      "       'data_consistency_result_x', 'data_comparison_result_x',\n",
      "       'police_record_result_x', 'compromised_document_result_x',\n",
      "       'properties_doc', 'sub_result_x', 'result',\n",
      "       'visual_authenticity_result', 'image_integrity_result_y',\n",
      "       'face_detection_result_y', 'image_quality_result_y',\n",
      "       'supported_document_result_y', 'conclusive_document_quality_result_y',\n",
      "       'colour_picture_result_y', 'data_validation_result_y',\n",
      "       'data_consistency_result_y', 'data_comparison_result_y',\n",
      "       'police_record_result_y', 'compromised_document_result_y', 'properties',\n",
      "       'sub_result_y', 'gender', 'nationality', 'document_type',\n",
      "       'date_of_expiry', 'issuing_country', 'issuing_date', 'issuing_state',\n",
      "       'document_version'],\n",
      "      dtype='object')\n",
      "INFO:root:Type of kyc_data: <class 'pandas.core.frame.DataFrame'>\n",
      "INFO:root:Columns in kyc_data: ['Unnamed: 0_facial', 'user_id', 'result_facial', 'face_comparison_result', 'created_at', 'facial_image_integrity_result', 'visual_authenticity_result_facial', 'properties_facial', 'attempt_id', 'Unnamed: 0_doc', 'result_doc', 'visual_authenticity_result_doc', 'image_integrity_result_x', 'face_detection_result_x', 'image_quality_result_x', 'created_at_doc', 'supported_document_result_x', 'conclusive_document_quality_result_x', 'colour_picture_result_x', 'data_validation_result_x', 'data_consistency_result_x', 'data_comparison_result_x', 'police_record_result_x', 'compromised_document_result_x', 'properties_doc', 'sub_result_x', 'result', 'visual_authenticity_result', 'image_integrity_result_y', 'face_detection_result_y', 'image_quality_result_y', 'supported_document_result_y', 'conclusive_document_quality_result_y', 'colour_picture_result_y', 'data_validation_result_y', 'data_consistency_result_y', 'data_comparison_result_y', 'police_record_result_y', 'compromised_document_result_y', 'properties', 'sub_result_y', 'gender', 'nationality', 'document_type', 'date_of_expiry', 'issuing_country', 'issuing_date', 'issuing_state', 'document_version', 'date']\n",
      "INFO:root:Data types of columns: Unnamed: 0_facial                                int64\n",
      "user_id                                         object\n",
      "result_facial                                   object\n",
      "face_comparison_result                          object\n",
      "created_at                                      object\n",
      "facial_image_integrity_result                   object\n",
      "visual_authenticity_result_facial               object\n",
      "properties_facial                               object\n",
      "attempt_id                                      object\n",
      "Unnamed: 0_doc                                   int64\n",
      "result_doc                                      object\n",
      "visual_authenticity_result_doc                  object\n",
      "image_integrity_result_x                        object\n",
      "face_detection_result_x                         object\n",
      "image_quality_result_x                          object\n",
      "created_at_doc                                  object\n",
      "supported_document_result_x                     object\n",
      "conclusive_document_quality_result_x            object\n",
      "colour_picture_result_x                         object\n",
      "data_validation_result_x                        object\n",
      "data_consistency_result_x                       object\n",
      "data_comparison_result_x                        object\n",
      "police_record_result_x                          object\n",
      "compromised_document_result_x                   object\n",
      "properties_doc                                  object\n",
      "sub_result_x                                    object\n",
      "result                                          object\n",
      "visual_authenticity_result                      object\n",
      "image_integrity_result_y                        object\n",
      "face_detection_result_y                         object\n",
      "image_quality_result_y                          object\n",
      "supported_document_result_y                     object\n",
      "conclusive_document_quality_result_y            object\n",
      "colour_picture_result_y                         object\n",
      "data_validation_result_y                        object\n",
      "data_consistency_result_y                       object\n",
      "data_comparison_result_y                        object\n",
      "police_record_result_y                          object\n",
      "compromised_document_result_y                   object\n",
      "properties                                      object\n",
      "sub_result_y                                    object\n",
      "gender                                          object\n",
      "nationality                                     object\n",
      "document_type                                   object\n",
      "date_of_expiry                                  object\n",
      "issuing_country                                 object\n",
      "issuing_date                                    object\n",
      "issuing_state                                   object\n",
      "document_version                               float64\n",
      "date                                    datetime64[ns]\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns available in perform_analysis: Index(['Unnamed: 0_facial', 'user_id', 'result_facial',\n",
      "       'face_comparison_result', 'created_at', 'facial_image_integrity_result',\n",
      "       'visual_authenticity_result_facial', 'properties_facial', 'attempt_id',\n",
      "       'Unnamed: 0_doc', 'result_doc', 'visual_authenticity_result_doc',\n",
      "       'image_integrity_result_x', 'face_detection_result_x',\n",
      "       'image_quality_result_x', 'created_at_doc',\n",
      "       'supported_document_result_x', 'conclusive_document_quality_result_x',\n",
      "       'colour_picture_result_x', 'data_validation_result_x',\n",
      "       'data_consistency_result_x', 'data_comparison_result_x',\n",
      "       'police_record_result_x', 'compromised_document_result_x',\n",
      "       'properties_doc', 'sub_result_x', 'result',\n",
      "       'visual_authenticity_result', 'image_integrity_result_y',\n",
      "       'face_detection_result_y', 'image_quality_result_y',\n",
      "       'supported_document_result_y', 'conclusive_document_quality_result_y',\n",
      "       'colour_picture_result_y', 'data_validation_result_y',\n",
      "       'data_consistency_result_y', 'data_comparison_result_y',\n",
      "       'police_record_result_y', 'compromised_document_result_y', 'properties',\n",
      "       'sub_result_y', 'gender', 'nationality', 'document_type',\n",
      "       'date_of_expiry', 'issuing_country', 'issuing_date', 'issuing_state',\n",
      "       'document_version', 'date'],\n",
      "      dtype='object')\n",
      "Overall KYC Pass Rate: 61.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of column selector: <class 'list'>\n",
      "INFO:root:Content of column selector: ['result_facial', 'result_doc']\n",
      "INFO:root:Groupby operation successful. Shape of daily_pass_rates: (161,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from pandas import Period\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "def setup_logging(run_dir):\n",
    "    current_time = datetime.now().strftime(\"%d%m%Y_%H%M\")\n",
    "    log_filename = os.path.join(run_dir, f\"debug_{current_time}.log\")\n",
    "    print(f\"Attempting to create log file at: {os.path.abspath(log_filename)}\")\n",
    "    \n",
    "    # Create a file handler\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Create a formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Get the root logger and add the file handler\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    root_logger.addHandler(file_handler)\n",
    "    \n",
    "    print(\"Logging setup complete\")\n",
    "\n",
    "def safe_execute(func):\n",
    "    \"\"\"Decorator to safely execute functions and log errors.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in {func.__name__}: {str(e)}\")\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "@safe_execute\n",
    "def time_series_decomposition(series, period):\n",
    "    if not isinstance(series.index, pd.DatetimeIndex):\n",
    "        series.index = pd.to_datetime(series.index)\n",
    "    result = seasonal_decompose(series, model='additive', period=period)\n",
    "    return result\n",
    "\n",
    "@safe_execute\n",
    "def check_stationarity(series):\n",
    "    result = adfuller(series)\n",
    "    return result[1] <= 0.05\n",
    "\n",
    "@safe_execute\n",
    "def rolling_statistics(series, window):\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    rolling_std = series.rolling(window=window).std()\n",
    "    return rolling_mean, rolling_std\n",
    "\n",
    "@safe_execute\n",
    "def correlation_analysis(kyc_data):\n",
    "    numeric_cols = kyc_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    if 'pass_rate' in kyc_data.columns:\n",
    "        numeric_cols.append('pass_rate')\n",
    "    \n",
    "    if 'days_to_expiry' in kyc_data.columns:\n",
    "        numeric_cols.append('days_to_expiry')\n",
    "    elif 'date_of_expiry' in kyc_data.columns and 'created_at' in kyc_data.columns:\n",
    "        kyc_data['days_to_expiry'] = (pd.to_datetime(kyc_data['date_of_expiry']) - pd.to_datetime(kyc_data['created_at'])).dt.days\n",
    "        numeric_cols.append('days_to_expiry')\n",
    "    \n",
    "    if 'age' in kyc_data.columns:\n",
    "        numeric_cols.append('age')\n",
    "    elif 'date_of_birth' in kyc_data.columns and 'created_at' in kyc_data.columns:\n",
    "        kyc_data['age'] = (pd.to_datetime(kyc_data['created_at']) - pd.to_datetime(kyc_data['date_of_birth'])).astype('<m8[Y]')\n",
    "        numeric_cols.append('age')\n",
    "    \n",
    "    numeric_cols = list(set(numeric_cols))\n",
    "    numeric_cols = [col for col in numeric_cols if col in kyc_data.columns]\n",
    "    \n",
    "    correlation_matrix = kyc_data[numeric_cols].corr()\n",
    "    return correlation_matrix\n",
    "\n",
    "@safe_execute\n",
    "def trend_analysis(series):\n",
    "    ma_short = series.rolling(window=7).mean()\n",
    "    ma_long = series.rolling(window=30).mean()\n",
    "    \n",
    "    x = np.arange(len(series))\n",
    "    y = series.values\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    \n",
    "    return {\n",
    "        'ma_short': ma_short,\n",
    "        'ma_long': ma_long,\n",
    "        'trend_line': p(x)\n",
    "    }\n",
    "\n",
    "@safe_execute\n",
    "def seasonal_patterns(series, period):\n",
    "    if not isinstance(series.index, pd.DatetimeIndex):\n",
    "        series.index = pd.to_datetime(series.index, errors='coerce')\n",
    "    series = series.dropna()  # Remove any NaT values\n",
    "    if len(series) == 0:\n",
    "        logging.warning(\"No valid dates for seasonal analysis\")\n",
    "        return pd.Series()\n",
    "    return series.groupby(series.index.to_period(period)).mean()\n",
    "\n",
    "@safe_execute\n",
    "def anomaly_detection(series, window=20, threshold=2):\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    rolling_std = series.rolling(window=window).std()\n",
    "    z_scores = (series - rolling_mean) / rolling_std\n",
    "    anomalies = np.abs(z_scores) > threshold\n",
    "    return anomalies\n",
    "\n",
    "@safe_execute\n",
    "def interdependency_analysis(kyc_data):\n",
    "    doc_type_pass_rates = kyc_data.groupby(['date', 'document_type'])[['result_facial', 'result_doc']].apply(calculate_pass_rate).unstack()\n",
    "    country_pass_rates = kyc_data.groupby(['date', 'issuing_country'])[['result_facial', 'result_doc']].apply(calculate_pass_rate).unstack()\n",
    "    \n",
    "    return {\n",
    "        'doc_type_pass_rates': doc_type_pass_rates,\n",
    "        'country_pass_rates': country_pass_rates\n",
    "    }\n",
    "\n",
    "def validate_dates(kyc_data):\n",
    "    if 'date' not in kyc_data.columns:\n",
    "        if 'created_at' in kyc_data.columns:\n",
    "            kyc_data['date'] = pd.to_datetime(kyc_data['created_at']).dt.date\n",
    "        else:\n",
    "            raise ValueError(\"Neither 'date' nor 'created_at' column found in the DataFrame\")\n",
    "    \n",
    "    kyc_data['date'] = pd.to_datetime(kyc_data['date'], errors='coerce')\n",
    "    invalid_dates = kyc_data['date'].isnull()\n",
    "    if invalid_dates.any():\n",
    "        logging.warning(f\"Found {invalid_dates.sum()} invalid dates. These will be excluded from analysis.\")\n",
    "    return kyc_data[~invalid_dates]\n",
    "\n",
    "# create markdown file\n",
    "def create_markdown_file(filename, content):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# load data\n",
    "def load_data(facial_similarity_path, doc_reports_path, properties_path):\n",
    "    facial_similarity = pd.read_csv(facial_similarity_path)\n",
    "    doc_reports = pd.read_csv(doc_reports_path)\n",
    "    properties = pd.read_csv(properties_path)\n",
    "    \n",
    "    logging.debug(f\"Columns in facial_similarity: {facial_similarity.columns}\")\n",
    "    logging.debug(f\"Columns in doc_reports: {doc_reports.columns}\")\n",
    "    \n",
    "    kyc_data = pd.merge(facial_similarity, doc_reports, on=['user_id', 'attempt_id'], suffixes=('_facial', '_doc'))\n",
    "    kyc_data = kyc_data.rename(columns={'created_at_facial': 'created_at'})\n",
    "    \n",
    "    logging.debug(f\"Columns after first merge: {kyc_data.columns}\")\n",
    "    \n",
    "    kyc_data = pd.merge(kyc_data, properties, on='user_id', how='left')\n",
    "    \n",
    "    logging.debug(f\"Final columns in kyc_data: {kyc_data.columns}\")\n",
    "    \n",
    "    return kyc_data\n",
    "\n",
    "# calculate pass rates\n",
    "def calculate_pass_rate(df):\n",
    "    if isinstance(df, pd.Series):\n",
    "        return df.mean()\n",
    "    total_attempts = df.shape[0]\n",
    "    passed_attempts = df[(df['result_facial'] == 'clear') & (df['result_doc'] == 'clear')].shape[0]\n",
    "    return passed_attempts / total_attempts if total_attempts > 0 else 0\n",
    "\n",
    "# get failure reason\n",
    "def get_failure_reason(row):\n",
    "    if row['result_facial'] != 'clear' and row['result_doc'] != 'clear':\n",
    "        return 'Both'\n",
    "    elif row['result_facial'] != 'clear':\n",
    "        return 'Facial Similarity'\n",
    "    elif row['result_doc'] != 'clear':\n",
    "        return 'Document Check'\n",
    "    else:\n",
    "        return 'Anomaly: Passed but Flagged as Failure'\n",
    "\n",
    "\n",
    "# standardise gender labels\n",
    "def standardize_gender(gender):\n",
    "    gender = str(gender).upper()\n",
    "    if gender in ['MALE', 'M']:\n",
    "        return 'Male'\n",
    "    elif gender in ['FEMALE', 'F']:\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def generate_new_visualizations(oct_2017_analysis, ts_decomposition, rolling_mean, rolling_std, \n",
    "                                trend_results, seasonal_patterns_result, anomalies, \n",
    "                                interdependencies, daily_pass_rates, run_dir):\n",
    "    # Create visualizations for October 2017 analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    oct_2017_analysis['daily_pass_rates'].plot()\n",
    "    plt.title('October 2017 Daily Pass Rates')\n",
    "    plt.savefig(os.path.join(run_dir, 'oct_2017_pass_rates.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Create visualizations for time series decomposition\n",
    "    ts_decomposition.plot()\n",
    "    plt.savefig(os.path.join(run_dir, 'ts_decomposition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Create visualizations for rolling statistics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(rolling_mean, label='Rolling Mean')\n",
    "    plt.plot(rolling_std, label='Rolling Std')\n",
    "    plt.legend()\n",
    "    plt.title('Rolling Statistics')\n",
    "    plt.savefig(os.path.join(run_dir, 'rolling_statistics.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Create visualizations for trend analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(trend_results['ma_short'], label='7-day MA')\n",
    "    plt.plot(trend_results['ma_long'], label='30-day MA')\n",
    "    plt.plot(trend_results['trend_line'], label='Trend Line')\n",
    "    plt.legend()\n",
    "    plt.title('Trend Analysis')\n",
    "    plt.savefig(os.path.join(run_dir, 'trend_analysis.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Create visualizations for seasonal patterns\n",
    "    seasonal_patterns_result.plot(kind='bar')\n",
    "    plt.title('Seasonal Patterns')\n",
    "    plt.savefig(os.path.join(run_dir, 'seasonal_patterns.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Create visualizations for anomaly detection\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_pass_rates)\n",
    "    plt.scatter(daily_pass_rates.index[anomalies], daily_pass_rates[anomalies], color='red', label='Anomalies')\n",
    "    plt.legend()\n",
    "    plt.title('Anomaly Detection')\n",
    "    plt.savefig(os.path.join(run_dir, 'anomaly_detection.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Create visualizations for interdependencies\n",
    "    interdependencies['doc_type_pass_rates'].plot(figsize=(12, 6))\n",
    "    plt.title('Pass Rates by Document Type Over Time')\n",
    "    plt.savefig(os.path.join(run_dir, 'doc_type_pass_rates_over_time.png'))\n",
    "    plt.close()\n",
    "\n",
    "    interdependencies['country_pass_rates'].plot(figsize=(12, 6))\n",
    "    plt.title('Pass Rates by Country Over Time')\n",
    "    plt.savefig(os.path.join(run_dir, 'country_pass_rates_over_time.png'))\n",
    "    plt.close()\n",
    "\n",
    "# perform analysis\n",
    "def perform_analysis(kyc_data, run_dir):\n",
    "    try:\n",
    "        print(\"Columns available in perform_analysis:\", kyc_data.columns)\n",
    "        logging.info(f\"Type of kyc_data: {type(kyc_data)}\")\n",
    "        logging.info(f\"Columns in kyc_data: {kyc_data.columns.tolist()}\")\n",
    "        logging.info(f\"Data types of columns: {kyc_data.dtypes}\")\n",
    "        if 'created_at_facial' in kyc_data.columns:\n",
    "            kyc_data['created_at'] = pd.to_datetime(kyc_data['created_at_facial'])\n",
    "        elif 'created_at_doc' in kyc_data.columns:\n",
    "            kyc_data['created_at'] = pd.to_datetime(kyc_data['created_at_doc'])\n",
    "        else:\n",
    "            raise ValueError(\"Neither 'created_at_facial' nor 'created_at_doc' found in the DataFrame\")\n",
    "        \n",
    "        kyc_data['date'] = kyc_data['created_at'].dt.date\n",
    "        kyc_data['date_of_expiry'] = pd.to_datetime(kyc_data['date_of_expiry'].replace('9999-01-01', pd.NaT), errors='coerce')\n",
    "        kyc_data['date_of_expiry'] = kyc_data['date_of_expiry'].dt.tz_localize('UTC')\n",
    "        kyc_data['gender'] = kyc_data['gender'].apply(standardize_gender)\n",
    "        kyc_data['failure_reason'] = kyc_data.apply(get_failure_reason, axis=1)\n",
    "\n",
    "        failure_counts = kyc_data['failure_reason'].value_counts()\n",
    "\n",
    "        kyc_data['created_at'] = pd.to_datetime(kyc_data['created_at'], utc=True)\n",
    "\n",
    "        overall_pass_rate = calculate_pass_rate(kyc_data)\n",
    "        print(f\"Overall KYC Pass Rate: {overall_pass_rate:.2%}\")\n",
    "\n",
    "        # Daily pass rates\n",
    "        daily_pass_rates = kyc_data.groupby('date')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        daily_pass_rates.plot()\n",
    "        plt.title('Daily KYC Pass Rates')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Pass Rate')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.subplots_adjust(bottom=0.2)  # Adjust the bottom margin\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(run_dir, f'daily_pass_rates.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Failure reasons\n",
    "        failure_counts = kyc_data[kyc_data['failure_reason'] != 'Passed']['failure_reason'].value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))  # Increase figure width\n",
    "        plt.bar(failure_counts.index, failure_counts.values)\n",
    "        plt.title('KYC Failure Reasons')\n",
    "        plt.xlabel('Reason')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate labels and align right\n",
    "        plt.tight_layout()  # Adjust layout to prevent cutting off labels\n",
    "        plt.savefig(os.path.join(run_dir, 'failure_reasons.png'), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Document types\n",
    "        doc_type_pass_rates = kyc_data.groupby('document_type')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "        \n",
    "        # Document type pass rates\n",
    "        plt.figure(figsize=(15, 8))  # Increase figure size\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate labels and align\n",
    "        plt.tight_layout()  # Adjust layout to fit labels\n",
    "        doc_type_pass_rates.plot(kind='bar')\n",
    "        plt.title('Pass Rates by Document Type')\n",
    "        plt.xlabel('Document Type')\n",
    "        plt.ylabel('Pass Rate')\n",
    "        plt.savefig(os.path.join(run_dir, 'doc_type_pass_rates.png'), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Issuing countries\n",
    "        country_pass_rates = kyc_data.groupby('issuing_country')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "        top_10_countries = country_pass_rates.nlargest(10)\n",
    "        bottom_10_countries = country_pass_rates.nsmallest(10)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        top_10_countries.plot(kind='bar')\n",
    "        plt.title('Top 10 Countries by KYC Pass Rate')\n",
    "        plt.xlabel('Issuing Country')\n",
    "        plt.ylabel('Pass Rate')\n",
    "        plt.savefig('top_10_countries.png')\n",
    "        plt.savefig(os.path.join(run_dir, f'top_10_countries.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Gender analysis\n",
    "        gender_pass_rates = kyc_data.groupby('gender')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        gender_pass_rates.plot(kind='bar')\n",
    "        plt.title('Pass Rates by Gender')\n",
    "        plt.xlabel('Gender')\n",
    "        plt.ylabel('Pass Rate')\n",
    "        plt.savefig(os.path.join(run_dir, f'gender_pass_rates.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Document expiry analysis\n",
    "        kyc_data['date_of_expiry'] = pd.to_datetime(kyc_data['date_of_expiry'])\n",
    "        kyc_data['days_to_expiry'] = (kyc_data['date_of_expiry'] - kyc_data['created_at']).dt.days\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))  # Increase figure size\n",
    "        sns.boxplot(x='failure_reason', y='days_to_expiry', data=kyc_data)\n",
    "        plt.title('Days to Document Expiry vs KYC Outcome')\n",
    "        plt.xlabel('KYC Outcome')\n",
    "        plt.ylabel('Days to Expiry')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate labels and align\n",
    "        plt.tight_layout()  # Adjust layout to fit labels\n",
    "        plt.savefig(os.path.join(run_dir, 'expiry_vs_outcome.png'), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=failure_counts.index, y=failure_counts.values)\n",
    "        plt.title('KYC Failure Reasons (Including Data Anomaly)')\n",
    "        plt.xlabel('Reason')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(run_dir, f'failure_reasons_with_anomaly.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Analyze October 2017 anomaly\n",
    "        oct_2017_analysis = analyze_october_2017_anomaly(kyc_data)\n",
    "        \n",
    "        # Perform time-series analysis\n",
    "        logging.info(f\"Type of column selector: {type(['result_facial', 'result_doc'])}\")\n",
    "        logging.info(f\"Content of column selector: {['result_facial', 'result_doc']}\")\n",
    "        try:\n",
    "            daily_pass_rates = kyc_data.groupby('date')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "            logging.info(f\"Groupby operation successful. Shape of daily_pass_rates: {daily_pass_rates.shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in groupby operation: {str(e)}\")\n",
    "            logging.info(f\"First few rows of kyc_data: {kyc_data.head().to_dict()}\")\n",
    "            raise\n",
    "        \n",
    "        # Decompose time series\n",
    "        ts_decomposition = time_series_decomposition(daily_pass_rates, period=30)\n",
    "        \n",
    "        # Check stationarity\n",
    "        is_stationary = check_stationarity(daily_pass_rates)\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        rolling_mean, rolling_std = rolling_statistics(daily_pass_rates, window=7)\n",
    "        \n",
    "        # Perform correlation analysis\n",
    "        corr_matrix = correlation_analysis(kyc_data)\n",
    "        \n",
    "        # Analyze trends\n",
    "        trend_results = trend_analysis(daily_pass_rates)\n",
    "        \n",
    "        # Identify seasonal patterns\n",
    "        seasonal_patterns_result = seasonal_patterns(daily_pass_rates, 'M')\n",
    "        \n",
    "        # Detect anomalies\n",
    "        anomalies = anomaly_detection(daily_pass_rates)\n",
    "        \n",
    "        # Analyze interdependencies\n",
    "        interdependencies = interdependency_analysis(kyc_data)\n",
    "\n",
    "        # Generate visualizations for new analyses\n",
    "        generate_new_visualizations(oct_2017_analysis, ts_decomposition, rolling_mean, rolling_std, \n",
    "                            trend_results, seasonal_patterns_result, anomalies, \n",
    "                            interdependencies, daily_pass_rates, run_dir)\n",
    "\n",
    "        # Compile all results\n",
    "        analysis_results = {\n",
    "            'overall_pass_rate': overall_pass_rate,\n",
    "            'daily_pass_rates': daily_pass_rates,\n",
    "            'failure_counts': failure_counts,\n",
    "            'doc_type_pass_rates': doc_type_pass_rates,\n",
    "            'top_10_countries': top_10_countries,\n",
    "            'bottom_10_countries': bottom_10_countries,\n",
    "            'gender_pass_rates': gender_pass_rates,\n",
    "            'oct_2017_analysis': oct_2017_analysis,\n",
    "            'is_stationary': is_stationary,\n",
    "            'correlation_matrix': corr_matrix,\n",
    "            'trend_results': trend_results,\n",
    "            'seasonal_patterns': seasonal_patterns_result,\n",
    "            'anomalies': anomalies,\n",
    "            'interdependencies': interdependencies\n",
    "        }\n",
    "\n",
    "        return analysis_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in perform_analysis: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")\n",
    "        raise  # Re-raise the exception after logging\n",
    "\n",
    "# generate conclusions\n",
    "def generate_conclusions(analysis_results):\n",
    "    conclusions = []\n",
    "    \n",
    "    if 'overall_pass_rate' in analysis_results:\n",
    "        conclusions.append(f\"The overall KYC pass rate is {analysis_results['overall_pass_rate']:.2%}. [daily_pass_rates.png]\")\n",
    "    \n",
    "    if 'daily_pass_rates' in analysis_results and len(analysis_results['daily_pass_rates']) > 1:\n",
    "        conclusions.append(f\"The pass rate has {'increased' if analysis_results['daily_pass_rates'].iloc[-1] > analysis_results['daily_pass_rates'].iloc[0] else 'decreased'} over time. [daily_pass_rates.png]\")\n",
    "    \n",
    "    if 'failure_counts' in analysis_results and not analysis_results['failure_counts'].empty:\n",
    "        conclusions.append(f\"The main reason for KYC failures is {analysis_results['failure_counts'].index[0]}. [failure_reasons.png]\")\n",
    "    \n",
    "    if 'doc_type_pass_rates' in analysis_results and not analysis_results['doc_type_pass_rates'].empty:\n",
    "        conclusions.append(f\"The document type with the highest pass rate is {analysis_results['doc_type_pass_rates'].idxmax()}. [doc_type_pass_rates.png]\")\n",
    "        conclusions.append(f\"The document type with the lowest pass rate is {analysis_results['doc_type_pass_rates'].idxmin()}. [doc_type_pass_rates.png]\")\n",
    "    \n",
    "    if 'top_10_countries' in analysis_results and not analysis_results['top_10_countries'].empty:\n",
    "        conclusions.append(f\"The country with the highest pass rate is {analysis_results['top_10_countries'].index[0]}. [top_10_countries.png]\")\n",
    "    \n",
    "    if 'bottom_10_countries' in analysis_results and not analysis_results['bottom_10_countries'].empty:\n",
    "        conclusions.append(f\"The country with the lowest pass rate is {analysis_results['bottom_10_countries'].index[0]}. [top_10_countries.png]\")\n",
    "    \n",
    "    if 'gender_pass_rates' in analysis_results and 'Male' in analysis_results['gender_pass_rates'] and 'Female' in analysis_results['gender_pass_rates']:\n",
    "        conclusions.append(f\"{'Male' if analysis_results['gender_pass_rates']['Male'] > analysis_results['gender_pass_rates']['Female'] else 'Female'} applicants have a higher pass rate. [gender_pass_rates.png]\")\n",
    "    \n",
    "    if 'is_stationary' in analysis_results:\n",
    "        conclusions.append(f\"The KYC pass rates {'are' if analysis_results['is_stationary'] else 'are not'} stationary over time.\")\n",
    "    \n",
    "    if 'correlation_matrix' in analysis_results and 'pass_rate' in analysis_results['correlation_matrix']:\n",
    "        corr_series = analysis_results['correlation_matrix']['pass_rate'].sort_values(ascending=False)\n",
    "        if len(corr_series) > 1:\n",
    "            conclusions.append(f\"The strongest correlation with pass rates is observed for {corr_series.index[1]}.\")\n",
    "    \n",
    "    if 'seasonal_patterns' in analysis_results and not analysis_results['seasonal_patterns'].empty:\n",
    "        try:\n",
    "            max_period = analysis_results['seasonal_patterns'].idxmax()\n",
    "            if isinstance(max_period, pd.Period):\n",
    "                period_str = max_period.strftime('%B %Y') if max_period.freq == 'M' else str(max_period)\n",
    "            else:\n",
    "                period_str = str(max_period)\n",
    "            conclusions.append(f\"Seasonal patterns show highest pass rates in {period_str}.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not determine highest seasonal pass rate: {str(e)}\")\n",
    "\n",
    "    if 'anomalies' in analysis_results:\n",
    "        conclusions.append(f\"{sum(analysis_results['anomalies'])} anomalies were detected in the pass rates time series.\")\n",
    "    \n",
    "    return conclusions\n",
    "\n",
    "# generate recommendations\n",
    "def generate_recommendations(analysis_results):\n",
    "    recommendations = []\n",
    "    \n",
    "    if 'failure_counts' in analysis_results and not analysis_results['failure_counts'].empty:\n",
    "        recommendations.append(f\"Focus on improving the {analysis_results['failure_counts'].index[0]} check process, as it's the main reason for failures. [failure_reasons.png]\")\n",
    "    \n",
    "    if 'doc_type_pass_rates' in analysis_results and not analysis_results['doc_type_pass_rates'].empty:\n",
    "        recommendations.append(f\"Enhance the verification process for {analysis_results['doc_type_pass_rates'].idxmin()} documents, which have the lowest pass rate. [doc_type_pass_rates.png]\")\n",
    "    \n",
    "    if 'bottom_10_countries' in analysis_results and not analysis_results['bottom_10_countries'].empty:\n",
    "        recommendations.append(f\"Investigate why {analysis_results['bottom_10_countries'].index[0]} has the lowest pass rate and implement country-specific improvements. [top_10_countries.png]\")\n",
    "    \n",
    "    recommendations.extend([\n",
    "        \"Provide clearer instructions to users on how to take high-quality photos for both documents and facial images. [failure_reasons.png]\",\n",
    "        \"Implement a feedback loop to continuously monitor and improve the KYC process. [daily_pass_rates.png]\"\n",
    "    ])\n",
    "    \n",
    "    if 'gender_pass_rates' in analysis_results and 'Male' in analysis_results['gender_pass_rates'] and 'Female' in analysis_results['gender_pass_rates']:\n",
    "        recommendations.append(f\"Address the gender disparity in pass rates, focusing on improving the process for {'female' if analysis_results['gender_pass_rates']['Male'] > analysis_results['gender_pass_rates']['Female'] else 'male'} applicants. [gender_pass_rates.png]\")\n",
    "    \n",
    "    recommendations.extend([\n",
    "        \"Consider implementing a system to remind users to update their documents well before expiry. [expiry_vs_outcome.png]\",\n",
    "        \"Develop a risk-based approach, potentially fast-tracking applications from low-risk countries and age groups. [top_10_countries.png]\",\n",
    "        \"Invest in machine learning models to predict KYC outcomes and identify high-risk applications early in the process. [failure_reasons.png]\",\n",
    "        \"Implement strategies to address seasonal variations in pass rates.\",\n",
    "        \"Investigate and address the root causes of detected anomalies in pass rates.\"\n",
    "    ])\n",
    "    \n",
    "    if 'interdependencies' in analysis_results and 'doc_type_pass_rates' in analysis_results['interdependencies']:\n",
    "        min_doc_type = analysis_results['interdependencies']['doc_type_pass_rates'].min().idxmin()\n",
    "        recommendations.append(f\"Focus on improving {min_doc_type} document verification process, which shows consistently lower pass rates.\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def analyze_october_2017_anomaly(kyc_data):\n",
    "    # Define the time range for analysis\n",
    "    start_date = '2017-10-01'\n",
    "    end_date = '2017-10-31'\n",
    "    \n",
    "    # Filter data for October 2017\n",
    "    oct_data = kyc_data[(kyc_data['created_at'] >= start_date) & (kyc_data['created_at'] <= end_date)]\n",
    "    \n",
    "    # Daily pass rates\n",
    "    daily_pass_rates = oct_data.groupby('date')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "    \n",
    "    # Analyze failure reasons\n",
    "    failure_reasons = oct_data[oct_data['failure_reason'] != 'Passed']['failure_reason'].value_counts()\n",
    "    \n",
    "    # Analyze by document type\n",
    "    doc_type_pass_rates = oct_data.groupby('document_type')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "    \n",
    "    # Analyze by country\n",
    "    country_pass_rates = oct_data.groupby('issuing_country')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "    \n",
    "    # Analyze by gender\n",
    "    gender_pass_rates = oct_data.groupby('gender')[['result_facial', 'result_doc']].apply(calculate_pass_rate)\n",
    "    \n",
    "    return {\n",
    "        'daily_pass_rates': daily_pass_rates,\n",
    "        'failure_reasons': failure_reasons,\n",
    "        'doc_type_pass_rates': doc_type_pass_rates,\n",
    "        'country_pass_rates': country_pass_rates,\n",
    "        'gender_pass_rates': gender_pass_rates\n",
    "    }\n",
    "\n",
    "# analysis\n",
    "def run_analysis(facial_similarity_path, doc_reports_path, properties_path):\n",
    "    logging.info(\"Starting run_analysis function\")\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%d%m%Y_%H%M\")\n",
    "    run_dir = f\"analysis_run_{current_time}\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    \n",
    "    setup_logging(run_dir)\n",
    "    \n",
    "    logging.info(\"Starting analysis\")\n",
    "    \n",
    "    # Initialize markdown_content at the beginning\n",
    "    markdown_content = \"# KYC Analysis Report\\n\\n\"\n",
    "    \n",
    "    try:\n",
    "        kyc_data = load_data(facial_similarity_path, doc_reports_path, properties_path)\n",
    "        kyc_data = validate_dates(kyc_data)\n",
    "        \n",
    "        analysis_results = perform_analysis(kyc_data, run_dir)\n",
    "        conclusions = generate_conclusions(analysis_results)\n",
    "        recommendations = generate_recommendations(analysis_results)\n",
    "\n",
    "        # Add conclusions and recommendations to markdown_content\n",
    "        markdown_content += \"## Conclusions\\n\\n\"\n",
    "        for i, conclusion in enumerate(conclusions, 1):\n",
    "            markdown_content += f\"{i}. {conclusion}\\n\"\n",
    "        \n",
    "        markdown_content += \"\\n## Recommendations\\n\\n\"\n",
    "        for i, recommendation in enumerate(recommendations, 1):\n",
    "            markdown_content += f\"{i}. {recommendation}\\n\"\n",
    "\n",
    "        # Time series analysis\n",
    "        try:\n",
    "            time_series_content = generate_time_series_analysis_content(analysis_results)\n",
    "            if time_series_content:\n",
    "                markdown_content += \"\\n## Time Series Analysis\\n\\n\"\n",
    "                markdown_content += time_series_content\n",
    "            else:\n",
    "                logging.warning(\"Time series analysis content generation returned empty string\")\n",
    "                markdown_content += \"\\n## Time Series Analysis\\n\\nTime series analysis unavailable.\\n\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating time series content: {str(e)}\")\n",
    "            markdown_content += \"\\n## Time Series Analysis\\n\\nError in time series analysis.\\n\"\n",
    "\n",
    "\n",
    "        # Create markdown content\n",
    "        markdown_content = \"# KYC Analysis Report\\n\\n\"\n",
    "        logging.info(\"Initialized markdown_content\")\n",
    "        \n",
    "        markdown_content += \"## Conclusions\\n\\n\"\n",
    "        for i, conclusion in enumerate(conclusions, 1):\n",
    "            markdown_content += f\"{i}. {conclusion}\\n\"\n",
    "        \n",
    "        markdown_content += \"\\n## Recommendations\\n\\n\"\n",
    "        for i, recommendation in enumerate(recommendations, 1):\n",
    "            markdown_content += f\"{i}. {recommendation}\\n\"\n",
    "        \n",
    "        markdown_content += \"\\n## Supporting Graphs\\n\\n\"\n",
    "        for graph in os.listdir(run_dir):\n",
    "            if graph.endswith('.png'):\n",
    "                markdown_content += f\"![{graph.split('.')[0]}]({graph})\\n\\n\"\n",
    "\n",
    "        markdown_content += \"\\n## October 2017 Anomaly Analysis\\n\\n\"\n",
    "        if 'oct_2017_analysis' in analysis_results:\n",
    "            markdown_content += generate_oct_2017_analysis_content(analysis_results['oct_2017_analysis'])\n",
    "        else:\n",
    "            markdown_content += \"October 2017 analysis not available.\\n\"\n",
    "\n",
    "        markdown_content += \"\\n## Time Series Analysis\\n\\n\"\n",
    "        markdown_content += generate_time_series_analysis_content(analysis_results)\n",
    "\n",
    "        markdown_content += \"\\n## Interdependency Analysis\\n\\n\"\n",
    "        if 'interdependencies' in analysis_results:\n",
    "            markdown_content += generate_interdependency_analysis_content(analysis_results['interdependencies'])\n",
    "        else:\n",
    "            markdown_content += \"Interdependency analysis not available.\\n\"\n",
    "\n",
    "        logging.info(f\"Total markdown content length: {len(markdown_content)}\")\n",
    "\n",
    "        # Create markdown file in the run directory\n",
    "        markdown_filename = os.path.join(run_dir, \"analysis_output.md\")\n",
    "        create_markdown_file(markdown_filename, markdown_content)\n",
    "\n",
    "        logging.info(f\"Analysis complete. Results written to {markdown_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in run_analysis: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")  # This will log the full stack trace\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "@safe_execute\n",
    "def generate_oct_2017_analysis_content(oct_2017_analysis):\n",
    "    content = \"\"\n",
    "    if 'daily_pass_rates' in oct_2017_analysis:\n",
    "        content += f\"Daily pass rates analysis for October 2017 is available.\\n\"\n",
    "    if 'failure_reasons' in oct_2017_analysis:\n",
    "        content += f\"Top failure reason: {oct_2017_analysis['failure_reasons'].index[0]}\\n\"\n",
    "    if 'doc_type_pass_rates' in oct_2017_analysis:\n",
    "        content += f\"Document type with highest pass rate: {oct_2017_analysis['doc_type_pass_rates'].idxmax()}\\n\"\n",
    "    if 'country_pass_rates' in oct_2017_analysis:\n",
    "        content += f\"Country with highest pass rate: {oct_2017_analysis['country_pass_rates'].idxmax()}\\n\"\n",
    "    if 'gender_pass_rates' in oct_2017_analysis:\n",
    "        content += f\"Gender with highest pass rate: {oct_2017_analysis['gender_pass_rates'].idxmax()}\\n\"\n",
    "    return content\n",
    "\n",
    "@safe_execute\n",
    "def generate_time_series_analysis_content(analysis_results):\n",
    "    logging.info(\"Starting time series analysis content generation\")\n",
    "    logging.debug(f\"Analysis results keys: {analysis_results.keys()}\")\n",
    "    content = \"\"\n",
    "\n",
    "    try:\n",
    "        if 'is_stationary' in analysis_results:\n",
    "            content += f\"The time series is {'stationary' if analysis_results['is_stationary'] else 'non-stationary'}.\\n\"\n",
    "\n",
    "        if 'trend_results' in analysis_results:\n",
    "            content += \"Trend analysis results are available.\\n\"\n",
    "\n",
    "        if 'seasonal_patterns' in analysis_results:\n",
    "            seasonal_patterns = analysis_results['seasonal_patterns']\n",
    "            if not seasonal_patterns.empty:\n",
    "                try:\n",
    "                    max_period = seasonal_patterns.idxmax()\n",
    "                    if isinstance(max_period, Period):\n",
    "                        period_str = max_period.strftime('%B %Y') if max_period.freq == 'M' else str(max_period)\n",
    "                    else:\n",
    "                        period_str = str(max_period)\n",
    "                    content += f\"Seasonal patterns analysis is available. Highest pass rates in {period_str}.\\n\"\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing seasonal patterns: {str(e)}\")\n",
    "                    content += \"Seasonal patterns analysis is available, but there was an error processing the results.\\n\"\n",
    "            else:\n",
    "                content += \"Seasonal patterns analysis is available, but no patterns were found.\\n\"\n",
    "\n",
    "        if 'anomalies' in analysis_results:\n",
    "            if isinstance(analysis_results['anomalies'], (list, np.ndarray)):\n",
    "                anomaly_count = sum(analysis_results['anomalies'])\n",
    "                content += f\"Anomaly detection found {anomaly_count} anomalies in the time series.\\n\"\n",
    "            else:\n",
    "                logging.warning(f\"Unexpected type for anomalies: {type(analysis_results['anomalies'])}\")\n",
    "                content += \"Anomaly detection results are available, but in an unexpected format.\\n\"\n",
    "\n",
    "        # Add detailed logging for troubleshooting\n",
    "        for key, value in analysis_results.items():\n",
    "            if isinstance(value, pd.Series) or isinstance(value, pd.DataFrame):\n",
    "                logging.debug(f\"Data type of '{key}': {type(value)}\")\n",
    "                logging.debug(f\"Index type of '{key}': {type(value.index)}\")\n",
    "                if len(value) > 0:\n",
    "                    logging.debug(f\"First element type of '{key}': {type(value.iloc[0])}\")\n",
    "                    logging.debug(f\"First element of '{key}': {value.iloc[0]}\")\n",
    "                    if isinstance(value.index[0], Period):\n",
    "                        logging.debug(f\"First index element of '{key}' is a Period object\")\n",
    "    except Exception as e:\n",
    "        error_info = traceback.extract_tb(sys.exc_info()[2])\n",
    "        filename, line_number, _, _ = error_info[-1]\n",
    "        logging.error(f\"Error in {filename}, line {line_number}: {str(e)}\")\n",
    "        content += f\"Error occurred during time series analysis content generation in {filename}, line {line_number}.\\n\"\n",
    "    \n",
    "    if not content:\n",
    "        content = \"No time series analysis content could be generated.\\n\"\n",
    "\n",
    "    logging.info(f\"Completed time series analysis content generation. Content length: {len(content)}\")\n",
    "    return content\n",
    "\n",
    "@safe_execute\n",
    "def generate_interdependency_analysis_content(interdependencies):\n",
    "    content = \"\"\n",
    "    if 'doc_type_pass_rates' in interdependencies:\n",
    "        content += \"Pass rates by document type over time are available.\\n\"\n",
    "    if 'country_pass_rates' in interdependencies:\n",
    "        content += \"Pass rates by country over time are available.\\n\"\n",
    "    return content\n",
    "\n",
    "run_analysis('facial_similarity_reports.csv', 'doc_reports.csv', 'properties.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
